<p align="center">
  <img src="https://raw.githubusercontent.com/kishoredb/kishoredb-research/main/assets/Brain.JPG" width="120" />
</p>

# üß† Human‚ÄìAI Interaction, Metacognition & Explainability  
### **Research Portfolio ‚Äî Kishore D. B.**

This page presents the **human-centred, cognitive science, and explainability aspects** of my research practice.  
While much of my GitHub highlights applied AI and engineering systems, a significant part of my work focuses on:

- **how humans interpret, trust, and respond to AI systems**  
- **metacognition in AI-assisted decision-making**  
- **agency, epistemic grounding, and cognitive diversity**  
- **human factors in high-stakes, AI-mediated workflows**  

These studies form the foundation of my long-term agenda:  
**to understand how human reasoning, values, and cognitive strategies shape AI adoption and how AI reshapes work, inclusion, and well-being.**

---

# üß© Research Identity

I work at the intersection of **Human‚ÄìAI Interaction**, **cognitive science**, **explainable AI**, **social behaviour**, and **applied ML**.  
My research questions emerge from lived professional experience: observing how people make sense of AI recommendations, how trust fluctuates, and how decisions change when AI enters the workflow.

### Core themes in my research:
- **Metacognition** (monitoring, calibration, confidence, self-regulation)  
- **Agency & epistemic dependence**  
- **Interpretability and human-understandable explanations**  
- **Cognitive diversity in AI-assisted workplaces**  
- **Human factors: workload, trust, transparency, inclusion**  
- **Behavioural modelling of AI adoption**

This work complements my 20+ years of experience in engineering, AI design, and real-world deployment.

---

# üìö Independent Research Programme (2021‚ÄìPresent)

A self-driven research programme exploring human reasoning, transparency, trust, and agency in AI-mediated environments.

### Key contributions:
- Designed and conducted **interviews, think-aloud protocols, and thematic analysis** on how professionals interpret AI explanations.
- Mixed-method investigations of **confidence calibration**, metacognitive strategies, and human reliance on AI.
- Analysed **cognitive diversity** in decision-making workflows using behavioural and observational data.
- Developed **behavioural data frameworks** for metacognitive self-regulation in high-stakes tasks.
- Created **Aegis**, a system enabling traceable, interpretable AI interactions for empirical study.
- Modelled **value-based adoption behaviours** (willingness‚Äìtrust‚Äìeffort trade-offs).
- Delivered talks and disclosures on **interpretability, human-centred AI**, and transparency.

---

# üîç Selected Human‚ÄìAI Research Projects

## 1. **Metacognitive Strategies in AI-Assisted Decision-Making**  
**Study initiated:** early 2023  
**Conceptual refinement and analysis:** continued through 2023‚Äì24 during recovery

**Focus:**  
How evaluators adjust decisions, calibrate confidence, and shift reliance when interacting with AI-generated suggestions.

**Methods:**  
Interview plans, behavioural coding schemes, think-aloud protocols, mixed-method analysis.

**Insights:**  
Metacognitive strategies strongly influence trust, override behaviour, and perceived agency.

---

## 2. **Interpretability & Human Agency in AI-Mediated Work (2022‚ÄìEarly 2023)**  
**Active study period:** 2022 to early 2023  
(No implied work after May 2023)

**Focus:**  
How explanation formats, uncertainty cues, and model transparency affect user agency and epistemic grounding.

**Methods:**  
Qualitative studies, cognitive walkthroughs, comparative explanation testing.

**Insights:**  
Clear explanations and uncertainty handling significantly shape user confidence and autonomy.

---

## 3. **CredScore Behavioural Evaluation (2022)**  
This behavioural evaluation was conducted as part of the award-winning CredScore XAI system.

**Focus:**  
Human understanding of model rationale and interpretability cues.

**Contributions:**  
- Designed behavioural evaluation workflows  
- Analysed how different users interpreted SHAP/LIME explanations  
- Studied trust, acceptance, and override patterns  

This created a **human-centred explainability layer**, complementing the technical system.

---

# üß† Theoretical Foundations

My research aligns with:

### **Cognitive Science**
- Metacognition (monitoring, calibration, control)  
- Cognitive load, uncertainty perception  
- Confidence modelling & judgement formation  

### **Philosophy of Agency**
- Practical, bounded, and epistemic agency  
- Value-based decision frameworks  
- Human autonomy in algorithmic workflows  

### **Responsible AI**
- Transparency obligations  
- Ethical inclusion  
- Decision support & human oversight  

---

# üî¨ Research Skills & Methodological Strengths

### **Empirical Human Research**
- Semi-structured interviews  
- Cognitive interviews  
- Thematic analysis & grounded theory  
- Survey design, psychometrics  
- Behavioural time-series analysis  
- Mixed-effects and longitudinal modelling  

### **Human‚ÄìAI Interaction Studies**
- Interpretability testing  
- Trust calibration  
- Cognitive diversity assessment  
- UX evaluation for AI explanations  

### **Quantitative & Computational**
- Python, R, SciPy, Statsmodels  
- Experimental modelling  
- Causal inference basics  
- Reproducible pipelines  

---

# üß≠ Connection to Engineering & AI Systems

Although grounded in human behaviour, this work is deeply connected to the **technical systems** in my portfolio:

| Human‚ÄìAI Question | System That Enables It |
|-------------------|------------------------|
| How do users interpret AI signals? | Aegis (traceability + semantic context) |
| How does confidence shift with explanations? | CredScore (SHAP/LIME UX experiments) |
| How does behaviour change under pressure? | Fraud Engine (high-stakes anomalies) |
| How do different cognition profiles handle AI? | SmartCare & financial workflows |

This dual background (engineering + cognitive research) is a major strength for interdisciplinary research.

---

# üîó Cross-Repository Navigation

### **Deep-Dive Case Studies**
- [Aegis Deep Dive](../deep-dives/aegis-deep-dive.md)  
- [CredScore Deep Dive](../deep-dives/credscore-deep-dive.md)  
- [Fraud Engine Deep Dive](../deep-dives/fraud-engine-deep-dive.md)  
- [SmartCare Deep Dive](../deep-dives/smartcare-deep-dive.md)

### **Architecture Files**
- [AEGIS Architecture](../../diagrams/aegis-architecture.md)  
- [CredScore Architecture](../../diagrams/credscore-architecture.md)  
- [Fraud Engine Architecture](../../diagrams/fraud-engine-architecture.md)  
- [SmartCare Architecture](../../diagrams/smartcare-architecture.md)

### **Methodology**
- [Research Methodology](../research-methodology.md)

### **Reproducibility**
- [Reproducibility Notes](../reproducibility/README.md)

---

# ü™ú Future Directions

My long-term research goal is to build a **coherent framework** connecting:

- human metacognition  
- value-based reasoning  
- AI transparency  
- real-world adoption behaviour  

And to design tools that improve:

- inclusion  
- well-being  
- workplace resilience  
- human trust & clarity  

---

# üß© Summary

This page captures the **human-centred half** of my research identity ‚Äî  
the part that explores how people think, interpret, trust, and act when AI enters their world.

It complements my engineering work and completes the picture of who I am as a researcher:  
technical, human-oriented, cross-disciplinary, and deeply reflective about the systems we build.

