<p align="center">
  <img src="https://raw.githubusercontent.com/kishoredb/kishoredb-research/main/assets/Brain.JPG" width="120" />
</p>

# üß† Human-Centered AI & Cognitive Research Profile  
**Author:** Kishore D. B.  
**Updated:** 2025-12-08  

This document summarizes my research experience across human‚ÄìAI interaction,  
metacognition, explainability, and cognitive factors in AI-mediated work.  
Dates have been adjusted to reflect real-world conditions, including my  
medical leave after May 2023. All study work described here refers to  
**design work, conceptual analysis, and pre-accident empirical efforts**.

---

## üå± 1. Metacognitive Strategies in AI-Assisted Decision-Making  
**Designed in early 2023; conceptual analysis continued during 2023‚Äì24**

This study focused on how human evaluators calibrate confidence and adjust  
decisions when interacting with assistive AI tools.

### Key contributions  
- Designed think-aloud protocols and early interview guides (pre-May 2023).  
- Conducted initial thematic analysis and conceptual modelling during recovery.  
- Developed a framework explaining how confidence, uncertainty, and  
  self-monitoring shape trust in AI outputs.  

### Related materials  
- **Research Methodology:**  
  [`/docs/research-methodology.md`](./research-methodology.md)  
- **Aegis Deep Dive:**  
  [`/docs/deep-dives/aegis-deep-dive.md`](./deep-dives/aegis-deep-dive.md)

---

## üß≠ 2. Interpretability & Human Agency in AI-Mediated Work  
**2022‚ÄìEarly 2023**

This work examined epistemic dependence, user agency, and cognitive grounding  
of trust in human‚ÄìAI interactions.

### Key contributions  
- Studied how professionals interpret AI explanations.  
- Conducted interviews and observational studies (pre-May 2023).  
- Mapped how agency changes depending on transparency and cognitive effort, and perceived control.  

### Related materials  
- **CredScore Deep Dive:**  
  [`/docs/deep-dives/credscore-deep-dive.md`](./deep-dives/credscore-deep-dive.md)  
- **Explainability Diagrams:**  
  [`/diagrams/credscore/credscore-explainability-flow.md`](../diagrams/credscore/credscore-explainability-flow.md)

---

## üí≥ 3. CredScore Behavioural Evaluation  
**2022 (evaluation studies designed & executed pre-2023)**

CredScore included structured behavioural-interpretive testing of model  
explanations for loan officers, credit analysts, and cross-cultural teams.

### Key contributions  
- Designed explainability questionnaires and comprehension tests.  
- Analysed cognitive differences in how users interpret SHAP-based reasoning.  
- Developed a hybrid evaluation approach combining rules + ML + behavioural signals.  

### Related materials  
- **CredScore Architecture:**  
  [`/diagrams/credscore-architecture.md`](../diagrams/credscore-architecture.md)  
- **Modeling Pipeline Diagram:**  
  [`/diagrams/credscore/credscore-modeling-pipeline.md`](../diagrams/credscore/credscore-modeling-pipeline.md)

---

# üß© Supporting Cross-Domain Research Foundations

## Human Factors, Cognitive Science & Philosophy
- Metacognition (monitoring, control, self-regulation).  
- Agency theory (practical, bounded, and epistemic agency).  
- Value-based decision theory and cognitive-effort models.  
- Ethics of AI‚Äîexplanation obligations, inclusion, dignity.

---

# üß™ Relevant Technical Skills for Human-Centered AI Research

### Empirical Research
- Semi-structured interviews, cognitive interviews.  
- Think-aloud studies and thematic analysis.  
- Behavioural time-series and mixed-effects models.  
- Survey design, psychometrics, validity checks.  

### Experimental & Computational Skills
- Python, R, SciPy, Statsmodels.  
- Interaction modelling and causal-inference basics.  
- Reproducible pipelines and experiment configuration.  

### Cross-Disciplinary Collaboration
- Engineering ‚Üî Cognitive Science ‚Üî Behavioural Economics.  
- Consortium-style communication and stakeholder alignment.

---

üïäÔ∏è How 23 Years of Industry Experience Shaped This Research

Much of this research grew naturally from my experience building AI systems
used by real people in high-pressure environments‚Äîcredit analysts, risk teams,
healthcare workers, and global engineering groups.

Across these roles, I observed how humans:

interpret AI signals differently based on experience and cognitive style,

struggle with uncertainty when explanations are unclear,

modify their decisions through metacognitive reasoning,

adjust trust dynamically in response to system behaviour, and

rely on a mix of intuition + evidence when making high-stakes choices.

These observations shaped my academic interest in metacognition, human
agency, interpretability, and responsible AI design.

---

# üîó Cross-Links to Relevant Repository Sections

## Deep-Dive Documents
- [`Aegis`](./deep-dives/aegis-deep-dive.md)  
- [`CredScore`](./deep-dives/credscore-deep-dive.md)  
- [`Fraud Engine`](./deep-dives/fraud-engine-deep-dive.md)  
- [`SmartCare`](./deep-dives/smartcare-deep-dive.md)

## Architecture Files
- [`Aegis`](../diagrams/aegis-architecture.md)  
- [`CredScore`](../diagrams/credscore-architecture.md)  
- [`Fraud Engine`](../diagrams/fraud-engine-architecture.md)  
- [`SmartCare`](../diagrams/smartcare-architecture.md)

## Diagram Suites  
- [`/diagrams/aegis/`](../diagrams/aegis/)  
- [`/diagrams/credscore/`](../diagrams/credscore/)  
- [`/diagrams/fraud-engine/`](../diagrams/fraud-engine/)  
- [`/diagrams/smartcare/`](../diagrams/smartcare/)  

## Methodology Diagrams  
- [`Research Methodology Loop`](../diagrams/methodology/research-methodology-loop.md)  
- [`Transparency Pipeline`](../diagrams/methodology/transparency-pipeline.md)  
- [`Reproducibility Cycle`](../diagrams/methodology/reproducibility-cycle.md)

## Reproducibility Section  
- [`/docs/reproducibility/README.md`](./reproducibility/README.md)  
- [`Evaluation Notes`](./reproducibility/validation-notes.md)

---

üåü Conclusion

This human-centered research program reflects my long-standing belief that
technology should support human clarity, confidence, and dignity‚Äîespecially
in environments where decisions matter deeply.

My goal is to continue developing methods, explanations, and systems that
bridge human reasoning with trustworthy AI, contributing meaningfully to
the future of responsible and human-aligned technology.



